---
title: "Variational Inference for Dirichlet Process Mixture Models"
author: "Michael Ellis"
date: "2022-02-16"
header-includes:
    - \usepackage{amsmath}
output: html_document
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="bayesian-inference" class="section level3">
<h3>Bayesian Inference</h3>
<p>The main goal of Bayesian statistical inference is to make probabilistic statements about unknown parameters or unobserved data. This is done by first specifying a joint probability distribution of all observed and unobserved quantities. This is often called the “full probability model” and denoted by <span class="math inline">\(p(\theta, y)\)</span>, where <span class="math inline">\(\theta\)</span> are the unknown parameters and <span class="math inline">\(y\)</span> is the observed data. Probability theory tell us that this distribution can be separated into two parts, <span class="math inline">\(p(\theta, y) = p(y \mid \theta) p(\theta)\)</span>, where <span class="math inline">\(p(y \mid \theta)\)</span> is referred to as the data distribution and <span class="math inline">\(p(\theta)\)</span> is referred to as the prior distribution of <span class="math inline">\(\theta\)</span>. The full probability model can then be used to calculate the posterior density, <span class="math inline">\(p(\theta \mid y)\)</span>. If we are also interested in making probabilistic statements about unobserved quantities, commonly denoted by <span class="math inline">\(\tilde{y}\)</span>, then we will also want to calculate the posterior predictive density, <span class="math inline">\(p(\tilde{y} \mid y)\)</span>. These posterior densities are what allow us to make probabilistic statements about <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\tilde{y}\)</span>.</p>
<p>In theory, we can calculate the posterior density of <span class="math inline">\(\theta\)</span> given the observed data <span class="math inline">\(y\)</span> using Bayes' rule,
<span class="math display">\[\begin{align}
    p(\theta \mid y) = \frac{p(\theta, y)}{p(y)} = \frac{p(y \mid \theta)p(\theta)}{p(y)}.
\end{align}\]</span>
In practice, this turns out to be a difficult problem due to the fact that the <span class="math inline">\(p(y)\)</span> term in the above formula must be calculated using integration,
<span class="math display">\[\begin{align}
    p(y) = \int p(y \mid \theta)p(\theta) \; d{\theta},
\end{align}\]</span>
and it is common for this integral to either not have a closed form solution or be intractable due to <span class="math inline">\(\theta\)</span> being high dimensional. The same problem arises when calculating the posterior posterior predictive density,
<span class="math display">\[\begin{align}
    p(\tilde{y} \mid y) &amp;= \int p(\tilde{y}, \theta \mid y) \; d\theta \\
    &amp;= \int p(\tilde{y} \mid \theta, y) p(\theta \mid y)  \; d\theta \\
    &amp;= \int p(\tilde{y} \mid \theta) p(\theta \mid y)  \; d\theta.
\end{align}\]</span>
A wide variety of numerical integration techniques have been developed to overcome this problem. In recent history, Monte Carlo methods (especially Markov chain Monte Carlo – MCMC) have become the most common approach. While MCMC methods have been widely successful they can still be notoriously slow. As a consequence other approaches, that attempt to avoid numeric integration to have better computational speed, such as variational approximations, have been developed.</p>
</div>
<div id="variational-inference" class="section level3">
<h3>Variational Inference</h3>
<p>Variational inference is an alternative approach that uses optimization instead of numeric integration. Variational inference searches a family of approximate densities, <span class="math inline">\(\mathcal{Q}\)</span>, to find a density that is “closest” to the target posterior density, <span class="math inline">\(p(\theta \mid y)\)</span>. Closest is usually defined as minimizing the Kullback-Leibler (KL) divergence. Let <span class="math inline">\(q\)</span> denote an approximate density from the family <span class="math inline">\(\mathcal{Q}\)</span> then the Kullback-Leibler divergence from <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span> is given by
<span class="math display">\[\begin{align}
    \text{KL}(q \mid \mid p) = - E_q \Big{(} \log \Big{(} \frac{p(\theta \mid y)}{q(\theta)} \Big{)} \Big{)}.
\end{align}\]</span></p>
<div id="evidence-lower-bound-elbo" class="section level4">
<h4>Evidence Lower Bound (ELBO)</h4>
<p>It follows that in variational inference the “optimal” approximate posterior density is the solution of the optimization problem</p>
<p><span class="math display">\[\begin{align}
    q^{*}(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\text{arg min}} \; \text{KL} (q \mid \mid p).
\end{align}\]</span>
However, notice the posterior density <span class="math inline">\(p(\theta \mid y)\)</span> that is difficult to calculate is in the equation from the KL divergence. It turns out we can rewrite the KL divergence in the following way,</p>
<p><span class="math display">\[\begin{align}
    \text{KL}(q \mid \mid p) 
    &amp;= - E_q \Big{[} \log \Big{(} \frac{p(\theta \mid y)}{q(\theta)} \Big{)} \Big{]} \\
    &amp;= E_q[\log q(\theta)] - E_q[\log p(\theta \mid y)] \\
    &amp;= E_q[\log q(\theta)] - E_q \Big{[}\log \frac{p(\theta, y)}{p(y)} \Big{]} \\
    &amp;= E_q[\log q(\theta)] - E_q [\log p(\theta, y)] + E_q [\log p(y)] \\
    &amp;= E_q[\log q(\theta)] - E_q [\log p(\theta, y)] + \log p(y).
\end{align}\]</span></p>
<p>It is clear that <span class="math inline">\(\log p(y)\)</span> is a constant and therefore can be dropped from the function we need to optimize. Dropping the <span class="math inline">\(\log p(y)\)</span> component and multiplying by -1 gives the Evidence Lower BOund or ELBO,</p>
<p><span class="math display">\[\begin{align}
    \text{ELBO}(q) = E_q [\log p(\theta, y)] - E_q[\log q(\theta)].
\end{align}\]</span></p>
<p>Maximizing the ELBO is equivalent to minimizing the KL divergence.</p>
</div>
<div id="the-mean-field-variational-family" class="section level4">
<h4>The Mean-Field Variational Family</h4>
<p>Now that the optimization problem is defined the next step is to define a set of possible densities to search over. As before we denote the set of approximate densities to search over by <span class="math inline">\(\mathcal{Q}\)</span>. One common way to define this set of densities is known as the mean-field variational family where the parameters are assumed to be independent. That is,
<span class="math display">\[\begin{align}
    q(\theta) = \prod_{i=1}^{m} q_i (\theta_i)
\end{align}\]</span>
where each <span class="math inline">\(q_j\)</span> is a density that is appropriate for the corresponding parameter. For example, if one of the unknown parameters, <span class="math inline">\(\theta_i\)</span>, is a mean then we can select <span class="math inline">\(q_i\)</span> to be a normal distribution with mean <span class="math inline">\(\mu_{\theta_i}\)</span> and variance <span class="math inline">\(\sigma^2_{\theta_i}\)</span>.</p>
</div>
<div id="coordinate-ascent-mean-field-variational-inference" class="section level4">
<h4>Coordinate Ascent Mean-Field Variational Inference</h4>
<p>Now that the optimization problem is specified the next step is to develop an algorithm to solve the optimization problem. A common approach is coordinate ascent variational inference (CAVI). Assume <span class="math inline">\(q(\theta) = \prod_{i=1}^{m} q_i (\theta_i)\)</span> and define <span class="math inline">\(q_{-i}(\theta_{-i}) = \prod_{j \ne i} q_j (\theta_j)\)</span> then CAVI follows from the observation that</p>
<p><span class="math display">\[\begin{align}
    \text{ELBO}(q) 
    &amp;= E_q \Big{[} \log \Big{(} \frac{p(\theta, y)}{q(\theta)} \Big{)} \Big{]} \\
    &amp;=  \int \log \Big{(} \frac{p(\theta, y)}{q(\theta)} \Big{)} q(\theta) \; d \theta \\
    &amp;= \int \log \Big{(} \frac{p(\theta, y)}{ \prod_{i=1}^m q_i(\theta_i)} \Big{)} \prod_{i=1}^m                 q_i(\theta_i) \; d \theta \\
    &amp;= \int \big{(} \log p(\theta, y) -  \sum_{i=1}^m \log q_i(\theta_i) \big{)} \prod_{i=1}^m                 q_i(\theta_i) \; d \theta \\
    &amp;= \int \big{(} \log p(\theta, y) -  \sum_{i=1}^m \log q_i(\theta_i) \big{)} \prod_{i=1}^m                 q_i(\theta_i) \; d \theta \\
    &amp;= \int q_{i}(\theta_i) \Big{[} \underbrace{\int q_{-i}(\theta_{-i}) \log p(\theta, y) \; d \theta_{-i}}_{\text{$E_{q_{-i}(\theta_{-i})}[\log p(\theta, y)]$}} \Big{]} \; d \theta_i - \int q_i (\theta_i) \log q_i (\theta_i) \; d \theta_i - \underbrace{\int q_{-i} (\theta_{-i}) \log q_{-i} (\theta_{-i}) \; d \theta_{-i}}_{\text{function of $\theta_{-1}$ only}} \\
    &amp;= \int q_{i}(\theta_i) E_{q_{-i}(\theta_{-i})}[\log p(\theta, y)] \; d \theta_i - \int q_i (\theta_i) \log q_i (\theta_i) \; d \theta_i + \text{constant} \\
    &amp;= \int q_{i}(\theta_i) \frac{E_{q_{-i}(\theta_{-i})}[\log p(\theta, y)]}{\log q_i (\theta_i)} \; d \theta_i + \text{constant} \\
    &amp;= \int q_{i}(\theta_i) \log \Big{(}\frac{\text{exp}\{E_{q_{-i}(\theta_{-i})}[\log p(\theta, y)] \}}{q_i (\theta_i)} \Big{)} \; d \theta_i + \text{constant}.
\end{align}\]</span>
Considered the <span class="math inline">\(\text{exp} \{ E_{q_{-i}(\theta_{-i})}[\log p(\theta, y)] \}\)</span> term as an unnormalized density of <span class="math inline">\(\theta_i\)</span>. Furthermore, notice that <span class="math inline">\(E_{q_{-i}(\theta_{-i})}[\log p(\theta, y)] \propto E_{q_{-i}(\theta_{-i})}[\log p(\theta_i \mid \cdot )]\)</span> where <span class="math inline">\(p(\theta_i \mid \cdot)\)</span>, commonly called the full conditional, is the density of <span class="math inline">\(\theta_i\)</span> conditional on all observed data <span class="math inline">\(y\)</span> and all unobserved parameters <span class="math inline">\(\theta_{-i}\)</span>. Then it follows that,
<span class="math display">\[\begin{align}
    \text{ELBO}(q)  
    &amp;= \int q_{i}(\theta_i) \log \Big{(}\frac{\text{exp} \{ E_{q_{-i}(\theta_{-i})}[\log p(\theta, y)] \}}{q_i (\theta_i)} \Big{)} \; d \theta_i + \text{constant} \\
    &amp;= \int q_{i}(\theta_i) \log \Big{(}\frac{E_{q_{-i}(\theta_{-i})}[\log p(\theta_i \mid \cdot)]}{q_i (\theta_i)} \Big{)} \; d \theta_i + \text{constant} \\ 
    &amp;= - \text{KL} (q_i \mid \tilde{p}) + \text{constant}.
\end{align}\]</span>
Where, <span class="math inline">\(\tilde{p}(\theta_i) \propto E_{q_{-i}(\theta_{-i})}[\log p(\theta_i \mid \cdot)]\)</span>. Which is maximized when <span class="math inline">\(q_i(\theta_i) \propto E_{q_{-i}(\theta_{-i})}[\log p(\theta_i \mid \cdot)]\)</span>. So, CAVI finds the “optimal” approximate posterior density by iteratively updating each <span class="math inline">\(q_i(\theta_i) \propto E_{q_{-i}(\theta_{-i})}[\log p(\theta_i \mid \cdot)]\)</span> for <span class="math inline">\(i = 1 \cdots m\)</span> until the ELBO is maximized. If conjugate priors are used for the hyperparameters then each <span class="math inline">\(q_i(\theta_i)\)</span> is a known closed form density and CAVI simply becomes iteratively updating the hyperparameters for each <span class="math inline">\(q_i(\theta_i)\)</span>.</p>
</div>
</div>
<div id="a-finite-mixture-of-gaussians" class="section level3">
<h3>A Finite Mixture of Gaussians</h3>
<p>Let <span class="math inline">\(\mathbf{y} = (y_1, y_2, \cdots, y_N)\)</span> be a sample from a finite mixture of <span class="math inline">\(K\)</span> Gaussian distributions. That is, each observation <span class="math inline">\(y_i\)</span> is assumed to be drawn from one of <span class="math inline">\(K\)</span> Gaussian distributions. Let <span class="math inline">\(p_k\)</span> define the proportion of observations that were drawn from Gaussian distribution <span class="math inline">\(k\)</span>. Then the joint distribution of <span class="math inline">\(\mathbf{y}\)</span> can be written as
<span class="math display">\[\begin{align}
    p(\mathbf{y} \mid \mathbf{p}, \mathbf{\mu}, \mathbf{\sigma^2}) 
    = \prod_{i=1}^N \sum_{k = 1}^K p_k N(y_i \mid \mu_k, \sigma^2_k).
\end{align}\]</span></p>
<p>This joint distribution can be represented in another way by introducing an unobserved vector of indicator variables, <span class="math inline">\(z_i = (z_{i1}, z_{i2}, \cdots, z_{ik})\)</span>, for each observation <span class="math inline">\(y_i\)</span>, such that
<span class="math inline">\(z_i \sim \text{multinomial}(1 ; p_1, p_2, \cdots, p_k)\)</span>. That is, <span class="math inline">\(z_{ik} = 1\)</span> if <span class="math inline">\(y_i\)</span> is drawn from Gaussian distribution <span class="math inline">\(k\)</span> and <span class="math inline">\(0\)</span> otherwise and <span class="math inline">\(p(z_{ik} = 1) = \phi_k\)</span>. Then it follows that the joint distribution of <span class="math inline">\(\mathbf{y}\)</span> and the unobserved vectors indicator variables <span class="math inline">\(\mathbf{z} = (z_1, z_2, \cdots z_N)\)</span> is given by
<span class="math display">\[\begin{align}
    p(\mathbf{y}, \mathbf{z} \mid \mathbf{p}, \mathbf{\mu}, \mathbf{\sigma^2}) = \prod_{i=1}^N \prod_{k = 1}^K (p_k N(y_i ; \mu_k, \sigma^2_k))^{z_{ik}}.
\end{align}\]</span>
One problem with using a finite mixture of Gaussian distributions is that the number of distributions to use is assumed to be fixed and known. More commonly, the number of Gaussian distributions, <span class="math inline">\(K\)</span>, is a hyperparameter that needs to be tuned or learned from the data.</p>
</div>
<div id="dirichlet-process-gaussian-mixture-models" class="section level3">
<h3>Dirichlet Process Gaussian Mixture Models</h3>
<p>Instead of assuming the number of Gaussian distributions, <span class="math inline">\(K\)</span>, is a fixed and known finite number we can assume the number of Gaussian distributions is countably infinite but the proportion of observations coming from most of the components is zero. That is, <span class="math inline">\(p_k = 0\)</span> for most of the Gaussian distributions.
This is commonly done by using a stick-breaking representation of a Dirichlet process. The stick-breaking representation assumes
<span class="math display">\[\begin{align}
    p_k = V_k \prod_{l &lt; k} (1 - V_l), \quad V_k \sim \text{Beta}(1, \lambda) \quad k = 1,2, \cdots.
\end{align}\]</span>
The intuition behind this representation is that we start with a stick of length, break off a stick of length, <span class="math inline">\(V_1\)</span>, drawn randomly from <span class="math inline">\(\text{Beta}(1, \lambda_{p})\)</span>, and set <span class="math inline">\(p_1 = V_1\)</span>. From the remaining stick of length <span class="math inline">\(1 - V_1\)</span> break off another stick of length, <span class="math inline">\(V_2\)</span>, drawn randomly from <span class="math inline">\(\text{Beta}(1, \lambda_{p})\)</span> and set <span class="math inline">\(p_2 = V_2(1 - V_1)\)</span>. As we continue this process the length of the remaining stick decreases exponentially so that <span class="math inline">\(p_k \approx 0\)</span> for large values of <span class="math inline">\(k\)</span>. Therefore, in practice, instead of using an infinite mixture, we can use a finite approximation to the stick-breaking representation by selecting a sufficiently large number of mixture components, <span class="math inline">\(K\)</span>. In this case the joint distribution of <span class="math inline">\(\mathbf{y}\)</span> and the unobserved vectors of indicator variables <span class="math inline">\(\mathbf{z}\)</span> has the same from as the finite mixture of Gaussian distributions
<span class="math display">\[\begin{align}
    p(\mathbf{y}, \mathbf{z} \mid \mathbf{p}, \mathbf{\mu}, \mathbf{\sigma^2}) 
    &amp;= \prod_{i=1}^N \prod_{k = 1}^{K} (p_k N(y_i ; \mu_k, \sigma^2_k))^{z_{ik}}
\end{align}\]</span>
with <span class="math inline">\(K\)</span> chosen to be sufficiently large. However, in this case the stick-breaking representation is used as a prior for <span class="math inline">\(\mathbf{p}\)</span>,
<span class="math display">\[\begin{align}
     p_k &amp;= V_k \prod_{j=1}^{k-1}(1 - V_j) \quad \text{where} \quad V_k \sim \text{Beta}(1, \lambda) \quad k = 1, 2, \cdots K-1.
\end{align}\]</span>
The last component, <span class="math inline">\(V_K\)</span>, is assumed to be one. It will be helpful to know that if <span class="math inline">\(V_k \sim \text{Beta}(1, \lambda)\)</span> then <span class="math inline">\(1-V_k \sim \text{Beta}(\lambda, 1)\)</span> and so it follows that
<span class="math display">\[\begin{align*}
p(p_k) = p(V_k) \prod_{j=1}^{k-1}p(1 - V_j) = \text{Beta}(V_k ; 1, \lambda) \prod_{j=1}^{k-1} \text{Beta}(V_j \; \lambda, 1) \quad k = 1, 2, \cdots K-1.
\end{align*}\]</span> We also assign conjugate prior and hyperprior distributions for each <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\sigma^2_k\)</span>, and <span class="math inline">\(\lambda\)</span>.
<span class="math display">\[\begin{align}
    \mu_k &amp;\sim N(\mu_{\mu_k}, \sigma^2_{\mu_k}) \quad k = 1, 2, \cdots K, \\
    \sigma^2_k &amp;\sim \text{IG}(\alpha_{\sigma^2_k}, \beta_{\sigma^2_k}) \quad k = 1, 2, \cdots K, \\
    \lambda &amp;\sim \text{gamma}(a_{\lambda}, b_{\lambda}).
\end{align}\]</span></p>
</div>
<div id="variational-inference-for-dirichlet-process-gaussian-mixture-models" class="section level3">
<h3>Variational Inference for Dirichlet Process Gaussian Mixture Models</h3>
<p>As the the CAVI algorithm requires assume the optimal density has the form
<span class="math display">\[\begin{align}
    q(\mathbf{z}, \lambda, \mathbf{V}, \mathbf{\mu}, \mathbf{\sigma}^2) 
    = q_{\mathbf{z}}(\mathbf{z}) q_{\lambda}(\lambda) \prod_{k=1}^K q_{\mu_k}(\mu_k) q_{\sigma^2_k}(\sigma^2_k) q_{V_k}(V_k).
\end{align}\]</span>
Since conjugate priors were chosen for each of the parameters and hyperparameters the corresponding optimal distributions have closed form and so can find equations to iteratively update each optimal density’s parameters in closed form.</p>
<div id="optimal-densities" class="section level4">
<h4>Optimal Densities</h4>
<p>Recal that each of the optimal densities is given by
<span class="math display">\[\begin{align*}
q_{\theta_i}(\theta_i) \propto \text{exp}\{E_{q(-\theta_i)} \log p(\theta_i \mid \cdot)\}.
\end{align*}\]</span>
Since we have chosen conjugate prior and hyperprior distributions each of the optimal densities can be calculated in close form.</p>
<div id="optimal-density-for-mathbfz" class="section level5">
<h5>Optimal density for <span class="math inline">\(\mathbf{z}\)</span></h5>
<p>For <span class="math inline">\(i = 1, 2, \cdots, N\)</span> and <span class="math inline">\(k = 1, 2, \cdots, K\)</span>,
<span class="math display">\[\begin{align*}
\log q_{z_{ik}}(z_{ik}) 
&amp;\propto E_{q(V_k, \mu_k, \sigma^2_k)} \log p(z_{ik} = 1 \mid \cdot) \\
&amp;= E_{q(V_k, \mu_k, \sigma^2_k)} \log p_k N(y_k ; \mu_k, \sigma^2_k) \\
&amp;= E_{q(V_k)}[\log p_k] - \frac{1}{2}\log(2\pi) - \frac{1}{2} E_{q(\sigma^2_k)}[\log \sigma^2_k] - \frac{1}{2 E_{q(\sigma^2_k)}[\log \sigma^2_k]} E_{q(\mu_k)}[(y_i - \mu_k)^2] \\
&amp;= \log \tilde{p}_{q(p_{ik})}.
\end{align*}\]</span>
It follows that
<span class="math display">\[\begin{align}
 q_{\mathbf{z}}(\mathbf{z}) = \text{multinomial}(N; \boldsymbol{p}_{q(\mathbf{p})}) \quad \text{where} \quad p_{q(p_{ik})} = \frac{\tilde{p}_{q(p_{ik})}}{\sum_{k=1}^{K} \tilde{p}_{q(p_{ik})}} \quad i = 1,2, \cdots N, \quad k = 1,2, \cdots K.
\end{align}\]</span></p>
</div>
<div id="optimal-density-for-mathbfmu_k" class="section level5">
<h5>Optimal density for <span class="math inline">\(\mathbf{\mu}_k\)</span></h5>
<p>For <span class="math inline">\(k = 1, 2, \cdots, K\)</span>,
<span class="math display">\[\begin{align}
    \log q_{\mu_k}(\mu_k) 
    &amp;\propto E_{q(\mathbf{z}, \mathbf{\sigma}^2_k)} \log p(\mu_k \mid \cdot) \\
    &amp;= E_{q(\mathbf{z}, \mathbf{\sigma}^2_k)} \sum_{i=1}^N \log N(y_i ; \mu_k, \sigma^2_k)^{z_{ik}} + \log N(\mu_k ; \mu_{\mu_k}, \sigma^2_{\mu_k}).
\end{align}\]</span>
It follows that,
<span class="math display">\[\begin{align}
q_{\mu_{k}}(\mu_{k}) &amp;= N(\mu_{q(\mu_k)}, \sigma^2_{q(\mu_k)}).
\end{align}\]</span>
Where,
<span class="math display">\[\begin{align}
\sigma^2_{q(\mu_k)} = \Big{(}\frac{1}{\sigma^2_0} + \frac{ \sum_{i=1}^N E_{q(z)}[z_{ik}]}{E_{q(\sigma^2_k)} [\sigma^2_k]} \Big{)}^{-1}, \quad
\mu_{q(\mu_k)} = \sigma^2_{q(\mu_k)} \Big{(} \frac{\mu_0}{\sigma^2_0} + \frac{\sum_{i=1}^N y_i E_{q(z)}[z_{ik}]}{E_{q(\sigma^2_k)} [\sigma^2_k]}\Big{)}.
\end{align}\]</span></p>
</div>
<div id="optimal-density-for-mathbfsigma2_k" class="section level5">
<h5>Optimal density for <span class="math inline">\(\mathbf{\sigma}^2_k\)</span></h5>
<p>For <span class="math inline">\(k = 1, 2, \cdots, K\)</span>,
<span class="math display">\[\begin{align}
    \log q_{\sigma^2_k}(\sigma^2_k) 
    &amp;\propto E_{q(\mathbf{z}, \mathbf{\mu}_k)} \log p(\sigma^2_k \mid \cdot) \\
    &amp;= E_{q(\mathbf{z}, \mathbf{\mu}_k)} \Big{[}\sum_{i=1}^N \log N(y_i ; \mu_k, \sigma^2_k)^{z_{ik}} + \log \text{Inv-gamma}(\sigma^2_k ; \alpha_{\sigma^2_k}, \beta_{\sigma^2_k})\Big{]}.
\end{align}\]</span>
It follows that,
<span class="math display">\[\begin{align}
q_{\sigma^2_{k}}(\sigma^2_{k}) = \text{Inv-gamma}\Big{(} A_{q(\sigma^2_k)}, B_{q(\sigma^2_k)} \Big{)}.
\end{align}\]</span>
Where,
<span class="math display">\[\begin{align}
     A_{q(\sigma^2_k)} = A + \frac{\sum_{i=1}^N E_{q(z)}[z_{ik}]}{2}, \quad B_{q(\sigma^2_k)} = B + \frac{\sum_{i=1}^N E_{q(z, \mu_k)}[(y_i - \mu_k)^2 z_{ik}]}{2}
\end{align}\]</span></p>
</div>
<div id="optimal-density-for-mathbfv_k" class="section level5">
<h5>Optimal density for <span class="math inline">\(\mathbf{V}_k\)</span></h5>
<p>For <span class="math inline">\(k = 1, 2, \cdots, K-1\)</span>,
<span class="math display">\[\begin{align}
    \log q_{V_k}(V_k) 
    &amp;\propto E_{q(z)} \log p(V_k \mid \cdot) \\
    &amp;= E_{q(z)} \Big{[} \sum_{i=1}^N \sum_{k=1}^K \log \text{Ber}(z_{ik}; V_k) + \log \text{Beta}(V_k ; 1, \lambda) \Big{]}.
\end{align}\]</span>
It follows that,
<span class="math display">\[\begin{align}
q_{V_{k}}(V_{k}) = \text{Beta}\Big{(}\alpha_{q(V_k)}, \beta_{q(V_k)} \Big{)}.
\end{align}\]</span>
Where,
<span class="math display">\[\begin{align}
\alpha_{q(V_k)} = 1 + \sum_{i=1}^N E_{q(z)}[z_{ik}], \quad \beta_{q(V_k)} =  E_{q(\lambda)}[\lambda] + \sum_{j = k+1}^{K} \sum_{i=1}^N E_{q(z)}[z_{ij}]
\end{align}\]</span></p>
</div>
<div id="optimal-density-for-lambda" class="section level5">
<h5>Optimal density for <span class="math inline">\(\lambda\)</span></h5>
<p><span class="math display">\[\begin{align}
    \log q_{\lambda}(\lambda) 
    &amp;\propto E_{q(V)} \log p(\lambda \mid \cdot) \\
    &amp;= E_{q(V)} \Big{[} \sum_{k=1}^K \log \text{Beta}(V_k; 1, \lambda) + \log \text{Gamma}(\lambda ; a_{\lambda}, b_{\lambda}) \Big{]}
\end{align}\]</span>
It follows that,
<span class="math display">\[\begin{align}
    q_{\lambda}(\lambda) = \text{Gamma}(\alpha_{q(\lambda)}, \beta_{q(\lambda)}).
\end{align}\]</span>
Where,
<span class="math display">\[\begin{align}
    \alpha_{q(\lambda)} = a_{\lambda} + N - 1, \quad \beta_{q(\lambda)} = b_{\lambda} - \sum_{k=1}^{K-1} E_{q(V)} \log (1 - V_k)
\end{align}\]</span></p>
</div>
<div id="elbo" class="section level5">
<h5>ELBO</h5>
We now calculate the ELBO for the Dirichlet Process Gaussian Mixture Model.
Recall the ELBO has the form
<span class="math display">\[\begin{align}
    \text{ELBO}(q) = E_q [\log p(\theta, y)] - E_q[\log q(\theta)].
\end{align}\]</span>
We start with the first term
<span class="math display">\[\begin{split}
E_q [ \log p(\mathbf{y}, \mathbf{z}, \mathbf{p}, \mathbf{V}, \lambda, \mathbf{\mu}, \mathbf{\sigma}^2)]  &amp;= \sum_{i=1}^N \sum_{k=1}^{K} E_q [\log p(y_i, z_i \mid \mu_k, \sigma^2_k, p_k)] + E_q [\log p(\mu_k)] + E_q [\log p(\sigma^2_k)] + E_q [\log p(\boldsymbol{p} \mid \lambda)]  + E_q[p(\lambda)] \\
    &amp;= \sum_{i=1}^N \sum_{k=1}^{K} E_q[z_{ik}] [E_q[\log p_k] - \log(2\pi)/2 - \log(\sigma^2_k)/2 - E_q[(y_i - \mu_k])^2]/(2E_q[\sigma^2_k]) \\ 
    &amp;+ \sum_{k=1}^K [(-1/2)(\log(2 \pi) + \log(\sigma^2_{\mu_k}) + \frac{1}{\sigma^2_{\mu_k}}E_q((\mu_k - \mu_{\mu_k})^2))] \\
    &amp;+ \sum_{k=1}^K [\alpha_{\sigma^2_k} \log \beta_{\sigma^2_k} - \log \Gamma(\alpha_{\sigma^2_k}) - (\alpha_{\sigma^2_k} + 1)E_q[\log (\sigma^2_k)] + \frac{\beta_{\sigma^2_k}}{E_q[\sigma^2_k]}] \\
    &amp;+\sum_{k=1}^K K [2 E_q[\log \Gamma(1 + \lambda)] - 2 E_q[\log \Gamma(\lambda)] + (E_q[\lambda]-1)E_q[\log (1-V_k)]] \\
    &amp;+ a_{\lambda} \log (b_{\lambda}) - \log \Gamma (a_\lambda) + (a_{\lambda} - 1) E_q[\log \lambda] - b_{\lambda} E_q[\lambda]
\end{split}\]</span>
Now the second term,
<span class="math display">\[\begin{split}
    E_q [ \log q(\mathbf{z}, \mathbf{V}, \lambda, \mathbf{\mu}, \mathbf{\sigma}^2)] 
    &amp;= \sum_{i=1}^N  E_q[\log q(z_{i})] + \sum_{k=1}^K E_q[\log q(\mu_k)] + \sum_{k=1}^K E_q[\log q(\sigma^2_k)] +  \sum_{k=1}^{K-1} E_q[\log q(V_k)] + E_q[\log q(\lambda)]  \\
    &amp;= \sum_{i=1}^N  \sum_{k=1}^K E_q[z_{ik}] \log p_{q(p_{ik})} \\
    &amp;+ \sum_{k=1}^K (-1/2)[\log(2 \pi) + \log(\sigma^2_{q(\mu_k)}) + \frac{1}{\sigma^2_{q(\mu_k)}}(E_q[(\mu_k - \mu_{q(\mu_k)})^2])]  \\
    &amp;+ \sum_{k=1}^K A_{q(\sigma^2_k)} \log \beta_{q(\sigma^2_k)} - \log \Gamma(A_{q(\sigma^2_k)}) - (A_{q(\sigma^2_k)} + 1)E_q[\log \sigma^2_k] - \frac{\beta_{q(\sigma^2_k)}}{E_q[\sigma^2_k]} \\
    &amp;+ \sum_{k=1}^{K-1} \log \Gamma(\alpha_{q(V_k)} + \beta_{q(V_k)}) - \log \Gamma(\alpha_{q(V_k)}) - \log \Gamma(\beta_{q(V_k)}) + (\alpha_{q(V_k)}-1)E_q[\log(V_k)] + (\beta_{q(V_k)}-1)E_q[\log(1 - V_k)] \\
    &amp;+ \alpha_{q(\lambda)} \log \beta_{q(\lambda)} - \log \Gamma(\alpha_{q(\lambda)}) + (\alpha_{q(\lambda)}-1)E_q[\log \lambda] - \beta_{q(\lambda)} E_q[\lambda]
\end{split}\]</span>
</div>
<div id="expectations" class="section level5">
<h5>Expectations</h5>
<p>The expectations need to update the parameters are simple to calculate since each of the optimal densities is known in closed form.
<span class="math display">\[\begin{align}
    E_{\mathbf{z}} [z_{ik}] &amp;= p_{q(p_{ik})}, \\
    E_{\mu_k} [\mu_k] &amp;= \mu_{q(\mu_k)}, \\
    E_{\sigma^2_k} [\sigma^2_k] &amp;= \frac{B_{q(\sigma^2_k)}}{A_{q(\sigma^2_k)}}, \\
    E_{\sigma^2_k} [\log \sigma^2_k] &amp;= \log (B_{q(\sigma^2_k)}) - \psi(A_{q(\sigma^2_k)}), \\
    E_{\mu_k}[(y_i - \mu_k)^2] &amp;= (y_i - \mu_{q(\mu_k)})^2 + \sigma^2_{q(\mu_k)}, \\
    E_q[(\mu_k - \mu_{\mu_k})^2] &amp;= (\mu_{q(\mu_k)} - \mu_{\mu_k})^2 + \sigma^2_{q(\mu_k)}, \\
    E_{\mathbf{V}} [\log (1 - V_k)] &amp;= \psi(\beta_{q(V_k)}) - \psi(\alpha_{q(V_k)} + \beta_{q(V_k)}), \\
    E_{\mathbf{p}}[\log p_k] &amp;= \psi(\alpha_{q(V_k)}) - \psi(\alpha_{q(V_k)} + \beta_{q(V_k)}) +  \sum_{j=1}^{k-1} [\psi(\beta_{q(V_j)}) - \psi(\alpha_{q(V_j)} + \beta_{q(V_j)})] \\
    E_q[\lambda] &amp;= \frac{\alpha_{q(\lambda)}}{\alpha_{q(\lambda)} + \beta_{q(\lambda)}} \\
    E_q[\log \lambda] &amp;= \psi(\alpha_{q(\lambda)}) - \log(\beta_{q(\lambda)}) \\
    E_q[\log \Gamma(1 + \lambda)] - E_q[\log \Gamma(\lambda)] &amp;= E_q[\log \Gamma(1 + \lambda) - \log \Gamma(\lambda)] = E_q[\log \lambda].
\end{align}\]</span>
Where <span class="math inline">\(\psi\)</span> denotes the digamma function.</p>
</div>
</div>
</div>
<div id="example" class="section level3">
<h3>Example</h3>
<p>As an example we will simulate data from a mixture of 3 Gaussian distributions. It is helpful to know that if <span class="math inline">\(V_k \sim \text{Beta}(1, \beta)\)</span> then <span class="math inline">\(1-V_k \sim \text{Beta}(\beta, 1)\)</span> and so it follows that
<span class="math display">\[\begin{align*}
p(\pi_k) = p(V_k) \prod_{j=1}^{k-1}p(1 - V_j) = \text{Beta}(V_k \mid 1, \beta) \prod_{j=1}^{k-1} \text{Beta}(V_j \mid \beta, 1).
\end{align*}\]</span></p>
<pre class="r"><code>## simulate data
set.seed(23)
N &lt;- 300 # number of samples to draw
mu &lt;- c(-5, 2, 10) # mean of each gaussian
sd &lt;- c(0.5, 1, 0.25) # standard deviation of each gaussian
probs &lt;- c(1/3, 1/3, 2/3) # probability of an observation being in each gaussian

# sample which gaussian each observation is in
x &lt;- sample(x = c(1,2,3), size = N, replace = TRUE, prob = probs) 
df &lt;- data.frame(
        x = factor(x),
        y = rnorm(N, mu[x], sd[x]) # sample observation given group
    )
    
ggplot(df, aes(x = y, color = x, fill = x)) + 
    geom_histogram() + 
    labs(title = &quot;Simulated Mixture of Gaussians&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="80%" /></p>
<pre><code>## simulate data
set.seed(23)
N &lt;- 100 # number of samples to draw
mu &lt;- c(-5, 2, 10) # mean of each gaussian
sd &lt;- c(0.5, 1, 0.25) # standard deviation of each gaussian
probs &lt;- c(1/3, 1/3, 2/3) # probability of an observation being in each gaussian

x &lt;- sample(x = c(1,2,3), size = N, replace = TRUE, prob = probs)
y &lt;- rnorm(N, mu[x], sd[x])

source(&quot;R/vi_dirichlet_mixture.R&quot;)
output &lt;- vi_dirichlet_mixture(
    y,
    K = 20,
    priors = NULL,
    max_iter = 100
) 

round(output$mu_q_mu, 2)</code></pre>
</div>
