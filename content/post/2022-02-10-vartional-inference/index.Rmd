---
title: "An Overview of Variational Inference"
author: "Michael Ellis"
date: "2022-02-16"
output: html_document
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{!htb}
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Bayesian Inference

A common goal of Bayesian statistical inference is to make probabilistic statements about unknown parameters or unobserved data. This is done by first specifying a joint probability distribution of all observed and unobserved quantities. This is often called the "full probability model" and denoted by $p(\theta, y)$, where $\theta$ are the unknown parameters and $y$ is the observed data. Probability theory tell us that this distribution can be separated into two parts, $p(\theta, y) = p(y \mid \theta) p(\theta)$, where $p(y \mid \theta)$ is referred to as the data distribution and $p(\theta)$ is referred to as the prior distribution of $\theta$. The full probability model can then be used to calculate the posterior density, $p(\theta \mid y)$. If we are also interested in making probabilistic statements about unobserved quantities, commonly denoted by $\tilde{y}$, then we will also want to calculate the posterior predictive density, $p(\tilde{y} \mid y)$. These posterior densities are what allow us to make probabilistic statements about $\theta$ and $\tilde{y}$.  

In theory, we can calculate the posterior density of $\theta$ given the observed data $y$ using Bayes\' rule,
\begin{align}
    p(\theta \mid y) = \frac{p(\theta, y)}{p(y)} = \frac{p(y \mid \theta)p(\theta)}{p(y)}.
\end{align}
In practice, this turns out to be a difficult problem due to the fact that the $p(y)$ term in the above formula must be calculated using integration,
\begin{align}
    p(y) = \int p(y \mid \theta)p(\theta) \; d{\theta},
\end{align}
and it is common for this integral to either not have a closed form solution or be intractable due to $\theta$ being high dimensional. The same problem arises when calculating the posterior posterior predictive density,
\begin{align}
    p(\tilde{y} \mid y) &= \int p(\tilde{y}, \theta \mid y) \; d\theta \\
    &= \int p(\tilde{y} \mid \theta, y) p(\theta \mid y)  \; d\theta \\
    &= \int p(\tilde{y} \mid \theta) p(\theta \mid y)  \; d\theta.
\end{align}
A wide variety of numerical integration techniques have been developed to overcome this problem. In recent history, Monte Carlo methods (especially Markov chain Monte Carlo -- MCMC) have become the most common approach. While MCMC methods have been widely successful they can still be notoriously slow. As a consequence other approaches that have better computational speed, such as variational approximations, have been developed.

### Variational Inference

Variational inference is an alternative approach where we search a family of approximate densities, $\mathcal{Q}$, to find a density that is "closest" to the target posterior density, $p(\theta \mid y)$. Closest is usually defined as minimizing the Kullback-Leibler (KL) divergence. Let $q$ denote an approximate density from the family $\mathcal{Q}$ then the Kullback-Leibler divergence from $p$ to $q$ is given by
\begin{align}
    \text{KL}(q \mid \mid p) = - E_q \Big{(} \log \Big{(} \frac{p(\theta \mid y)}{q(\theta)} \Big{)} \Big{)}.
\end{align}

#### Evidence Lower Bound (ELBO)

The "optimal" approximate posterior density is the solution of the optimization problem

\begin{align}
    q^{*}(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\text{arg min}} \; \text{KL} (q \mid \mid p).
\end{align}
However, notice the posterior density $p(\theta \mid y)$ that is difficult to calculate because we need to calculate $p(y)$ using integration is in the equation from the KL divergence. It turns out we can rewrite the KL divergence in the following way,

\begin{align}
    \text{KL}(q \mid \mid p) 
    &= - E_q \Big{[} \log \Big{(} \frac{p(\theta \mid y)}{q(\theta)} \Big{)} \Big{]} \\
    &= E_q[\log q(\theta)] - E_q[\log p(\theta \mid y)] \\
    &= E_q[\log q(\theta)] - E_q \Big{[}\log \frac{p(\theta, y)}{p(y)} \Big{]} \\
    &= E_q[\log q(\theta)] - E_q [\log p(\theta, y)] + E_q [\log p(y)] \\
    &= E_q[\log q(\theta)] - E_q [\log p(\theta, y)] + \log p(y).
\end{align} 

It is clear now that $\log p(y)$ is a constant and therefore can be dropped from the function we need to optimize. Dropping the $\log p(y)$ component and multiplying by -1 gives the Evidence Lower BOund or ELBO,

\begin{align}
    \text{ELBO}(q) = E_q [\log p(\theta, y)] - E_q[\log q(\theta)].
\end{align}

Maximizing the ELBO is equivalent to minimizing the KL divergence.

#### The Mean-Field Variational Family

Now that the optimization problem is defined the next step is to define a set of possible densities to search over. As before we denote the set of approximate densities to search over by $\mathcal{Q}$. One common way to define this set of densities is known as the mean-field variational family where the parameters are assumed to be independent. That is,
\begin{align}
    q(\theta \mid \phi) = \prod_{i=1}^{m} q_i (\theta_i \mid \phi_i)
\end{align}
where each $q_j$ is a density, with hyperparameters $\phi_i$, that is appropriate for the corresponding parameter. For example, if there is a single unknown parameter $\theta$ is a mean then we can select $q$ to be a normal distribution with mean $\mu_{\theta}$ and variance $\sigma^2_{\theta}$.

#### Coordinate Ascent Mean-Field Variational Inference

Now that the optimization problem is specified the next step is to develop an algorithm to solve the optimization problem. A common approach is coordinate ascent variational inference (CAVI). Assume  $q(\theta) = \prod_{i=1}^{m} q_i (\theta_i)$ and define $q_{-i}(\theta_{-i}) = \prod_{j \ne i} q_j (\theta_j)$ then CAVI follows from the observation that

\begin{align}
    \text{ELBO}(q) 
    &= E_q \Big{[} \log \Big{(} \frac{p(\theta, y)}{q(\theta)} \Big{)} \Big{]} \\
    &=  \int \log \Big{(} \frac{p(\theta, y)}{q(\theta)} \Big{)} q(\theta) \; d \theta \\
    &= \int \log \Big{(} \frac{p(\theta, y)}{ \prod_{i=1}^m q_i(\theta_i)} \Big{)} \prod_{i=1}^m                 q_i(\theta_i) \; d \theta \\
    &= \int \big{(} \log p(\theta, y) -  \sum_{i=1}^m \log q_i(\theta_i) \big{)} \prod_{i=1}^m                 q_i(\theta_i) \; d \theta \\
    &= \int \big{(} \log p(\theta, y) -  \sum_{i=1}^m \log q_i(\theta_i) \big{)} \prod_{i=1}^m                 q_i(\theta_i) \; d \theta \\
    &= \int q_{i}(\theta_i) \Big{[} \int q_{-i}(\theta_{-i}) \log p(\theta, y) \; d \theta_{-i} \Big{]} \; d \theta_i - \int q_i (\theta_i) \log q_i (\theta_i) \; d \theta_i - \int q_{-i} (\theta_{-i}) \log q_{-i} (\theta_{-i}) \; d \theta_{-i} \\
    &= \int q_{i}(\theta_i) E_{q_{-i}} [\log p(\theta, y)] \; d \theta_i - \int q_i (\theta_i) \log q_i (\theta_i) \; d \theta_i - \int q_{-i} (\theta_{-i}) \log q_{-i} (\theta_{-i}) \; d \theta_{-i} \\
     &= \int q_{i}(\theta_i) \frac{E_{q_{-i}} [\log p(\theta, y)]}{\log q_i (\theta_i)} \; d \theta_i - \int q_{-i} (\theta_{-i}) \log q_{-i} (\theta_{-i}) \; d \theta_{-i} \\
     &= \int q_{i}(\theta_i) \log \frac{ \tilde{p}(\theta_i, y) }{q_i (\theta_i)} \; d \theta_i - \int q_{-i} (\theta_{-i}) \log q_{-i} (\theta_{-i}) \; d \theta_{-i} \\
\end{align} 

where, $\tilde{p}(\theta_i, y) = \text{exp} \{ E_{q_{-i}} [\log p(\theta, y)] \}$. Notice, the last term is constant with respect to $\theta_i$ since it is a function of only $\theta_{-i}$ and the first term is the negative of the Kullback-Leibler divergence of $q_i$ with respect to $\tilde{p}(\theta_i, y)$. Therefore, for each $q_i(\theta_i)$ conditional on $q_{-i}(\theta_{-i})$,
\begin{align}
    \text{ELBO}(q)  = - \text{KL}(q_i \mid \tilde{p}(\theta_i, y)) + \text{constant}.
\end{align}
is maximized when $q_i(\theta_i) \equiv \frac{\tilde{p}(\theta_i, y)}{\int \tilde{p}(\theta_i, y) \; d \theta_i} = \tilde{p}(\theta_i \mid y)$. It follows CAVI is simply iteratively updating $q_i(\theta_i) = \tilde{p}(\theta_i \mid y)$. If conjugate priors are used for the hyperparameters, $\phi$, then each $q_i(\theta_i)$ is a known closed form density and CAVI simply becomes iteratively updating the hyperparameters, $\phi_i$ for each corresponding $q_i(\theta_i)$.

